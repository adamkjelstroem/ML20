{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Week 5 Exercises\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 1: Break Points and Growth Functions \n",
    "\n",
    "1.  Is there always a break point for a finite hypothesis set of $n$\n",
    "    hypotheses? If so, can you give an upper bound? What is the growth\n",
    "    function?\n",
    "\n",
    "2.  Does the set of all functions have a break point? What is its growth\n",
    "    function?\n",
    "\n",
    "3.  What is the (smallest) break point for the hypothesis set consisting\n",
    "    of circles centered around $(0,0)$? For a given circle the\n",
    "    hypothesis returns $1$ for points inside the circle and $-1$ for\n",
    "    points outside. What is the growth function?\n",
    "\n",
    "4.  What if we move to centered balls in the 3-dimensional space\n",
    "    ${{\\mathbb R}}^3$? Or in general $d$-dimensional space\n",
    "    ${{\\mathbb R}}^d$ (hyperspheres)?\n",
    "\n",
    "5. Show that the growth function for a singleton hypothesis class $H = \\{h\\}$ is 1\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1\n",
    "\n",
    "for each shattering, there needs to be a corresponding hypothesis. for $k$ data points, we have $2^k$ shatterings. when $2^k>n$, we are guaranteed to reach a break point. since the growth function is the max number of distinct ways to classify the points, but we know we only have n distinct ways, then $n$ is an upper bound on the growth function.\n",
    "\n",
    "## 1.2\n",
    "\n",
    "for each dichotomy of some (finite, distinct) points, there exists a function in the set of all functions producing exactly that dichotomy. so this set has no break point. Thus its growth function is $2^n$.\n",
    "\n",
    "## 1.3\n",
    "\n",
    "a hypothesis is uniquely defined by the non-negative number that is the radius of the circle. membership of the set defined by the hypothesis means distance to the origin being less than or equal to the radius. this means that if some point p at a distance d from the origin is in the set, then any point q at a distance $d'<d$ is also in the set. Similarly, if q is not in the set, then p cannot be either. Since membership for any two points $p$ and $q$ needs to be independent for all 4 dichotomies of 2 points to be attainable, and we have just shown that they are not, then 2 is a break point.\n",
    "growth function is $\\leq n+1$\n",
    "\n",
    "## 1.4\n",
    "\n",
    "membership is still dependent on whether distance to the origin is less than or equal to the radius of the $d$-dimensional hypersphere. thus the same argument holds from 1.3. 2 is still a break point.\n",
    "\n",
    "## 1.5\n",
    "\n",
    "see definition of growth function:\n",
    "\n",
    "$m_H(n)=max_{x_1,...x_n\\in X} |\\{(h(x_1, ..., x_n)): h \\in H\\}|$\n",
    "\n",
    "let $H={h_1}$. Then\n",
    "$m_H(n)=max_{x_1,...x_n\\in X} |\\{(h(x_1, ..., x_n)): h \\in \\{h_1\\}\\}| = max_{x_1,...x_n\\in X} |\\{h_1\\}| = max_{x_1,...x_n\\in X} 1 = 1$\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 2: VC Dimension \n",
    "\n",
    "1.  Does VC Dimension depend on the learning algorithm or the actual\n",
    "    data set given?\n",
    "\n",
    "2.  Does VC Dimension depend on the probability distribution generating\n",
    "    the data (not the labels)?\n",
    "\n",
    "3.  If $\\mathcal{H}_1 \\subseteq \\mathcal{H}_2$ is\n",
    "    $VC(\\mathcal{H}_1) \\leq VC(\\mathcal{H}_2)?$\n",
    "\n",
    "4.  Can you give an upper bound on the VC dimension of a finite set of\n",
    "    $M$ hypotheses?\n",
    "\n",
    "5.  What is the VC Dimension for the hypothesis set consisting of\n",
    "    circles centered around 0?\n",
    "\n",
    "6.  What if we move to balls (3d)? or in general d dimensions\n",
    "    (hypershperes)?\n",
    "\n",
    "7.  What is the maximal VC dimension possible of the intersection of\n",
    "    hypothesis spaces $\\mathcal{H}_1,\\dots,\\mathcal{H}_n$ with VC\n",
    "    dimension $v_1,\\dots,v_n$.\n",
    "\n",
    "8.  As previous question, instead what is the minimal VC dimension of\n",
    "    the union of the hypothesis spaces from the previous question\n",
    "\n",
    "9.  Show that the VC dimension the hypothesis set consisting of axis aligned rectangles in $\\mathbb{R}^2$ is 4, i.e. find a point set of 4 points you can shatter and argue that any point set of size 5 can not.\n",
    "    \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 and 2.2\n",
    "\n",
    "[slide 10](https://brightspace.au.dk//content/enforced/27850-LR3324/csfiles/home_dir/Slides/Lec1Part2(3).pdf?_&d2lSessionVal=WCqkhr6ojfNSvYi6f8d8zc2tO&ou=27850) states that\n",
    "\"d is the largest integer s.t. there is a data set of size d that can be shattered by H\"\n",
    "\n",
    "Thus $VC(H)$ is a property of $H$ and depends ONLY on $H$.\n",
    "\n",
    "## 2.3\n",
    "\n",
    "Assume $H_1\\subseteq H_2$. Since the hypotheses in $H_1$ are enough to shatter at least one data set of size $VC(H_1)$, and all members of $H_1$ are also in $H_2$, then $H_2$ can also shatter at least one data set of size $VC(H_2)$, but it may also be able to shatter a bigger one.\n",
    "Thus $VC(H_1)\\leq VC(H_2)$.\n",
    "\n",
    "## 2.4, \n",
    "\n",
    "same as 1.1. solve $2^k>n$ for k: $k=log_2(n)$ Thus $VC(H)=log_2(n)$ (plus/minus 1)\n",
    "\n",
    "## 2.5, 2.6\n",
    "\n",
    "Here $k=2$, so $d=1$ in both cases.\n",
    "\n",
    "## 2.7\n",
    "\n",
    "Let $H$ be the intersection of hypothesis sets $H_1, ..., H_n$. \n",
    "Then $\\forall i \\in [n]: H \\subseteq H_i$.\n",
    "From the property proven in 2.3 we then have that $\\forall i \\in [n]: VC(H)\\leq VC(H_i)$.\n",
    "So $VC(H)\\leq min_{i\\in[n]} VC(H_i)$.\n",
    "\n",
    "## 2.8\n",
    "\n",
    "Let $H$ be the union of hypothesis sets $H_1, ..., H_n$. \n",
    "Then $\\forall i \\in [n]: H_i \\subseteq H$.\n",
    "From the property proven in 2.3 we then have that $\\forall i \\in [n]: VC(H_i)\\leq VC(H)$.\n",
    "So $VC(H)\\geq max_{i\\in[n]} VC(H_i)$.\n",
    "\n",
    "## 2.9\n",
    "\n",
    "Assume $k=4$:\n",
    "\n",
    "let [(0,1),(1,0),(2,1),(1,2)] be such a set. Show by demonstration.\n",
    "\n",
    "Assume $k=5$.\n",
    "\n",
    "Let the data points be called $a, b, c, d, e$.\n",
    "Assume wlog that $a_x \\leq b_x \\leq c_x \\leq d_x \\leq e_x$.\n",
    "\n",
    "Assume for contradiction that we can shatter these 5. \n",
    "\n",
    "this means we can construct $\\{a, e\\}$. This means that either $a_y, e_y > b_y, c_y, d_y$ or $a_y, e_y < b_y, c_y, d_y$. \n",
    "Assume wlog that $a_y, e_y > b_y, c_y, d_y$.\n",
    "\n",
    "We can also construct $\\{a, b, e\\}$. \n",
    "This means that $b_y > c_y, d_y$.\n",
    "\n",
    "But we can also construct $\\{a, c, e\\}$. \n",
    "This means that $c_y > b_y, d_y$.\n",
    "\n",
    "now we have that $b_y > c_y$ and $c_y > b_y$, which is a contradicton. \n",
    "Thus the break point is $5$, so $VC(H)=4$, as desired.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 3: VC Dimension of Hyperplanes (Book Exercise 2.4 p. 52)\n",
    "Consider the input space $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$ (with the first coordinate being the constant 1). Show that the VC dimension of the hypothesis space $\\mathcal{H} = \\{\\textrm{sign}(w^\\intercal x) \\mid w\\in \\mathbb{R}^{d+1} \\}$ corresponding to the perceptron is $d+1$.\n",
    "\n",
    "We need to show \n",
    "1. That there exists a data set of size d+1 that can be shattered by hyperplanes\n",
    "2. That no data set of size d+2 can be shattered by hyperplanes\n",
    "\n",
    "We will give a few more hints than the book does.\n",
    "### Shattering d+1 points\n",
    "As the book hints you must create an \"easy\" data set that you store in matrix $X$. \n",
    "\n",
    "**Hint:** We suggest you consider as a data matrix, the $(d+1) \\times (d+1)$ matrix $X$ whose first column is all-1s (required since $\\mathcal{X} = \\{1\\} \\times \\mathbb{R}^d$) and where the lower $d \\times d$ corner of the matrix is the $d \\times d$ identity matrix.\n",
    "\n",
    "Show that you can construct any dichotomy $y \\in \\{-1,+1\\}^{d+1}$ using some $h \\in \\mathcal{H}$ and the data matrix $X$ defined above.\n",
    "\n",
    "\n",
    "### No Shattering of d+2 points.\n",
    "Must show that for any d+2 points we must prove there is a  dichotomy hyperplanes can not capture.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Consider an arbitrary set of d+2 points $x_1,\\dots, x_{d+2}$ of dimension (d+1) and think of them as vectors in $\\{1\\} \\times \\mathbb{R}^d \\subset \\mathbb{R}^{d+1}$.\n",
    "Since we have more vectors than dimensions the vectors must be linearly dependent.\n",
    "\n",
    "i.e. \n",
    "$$\n",
    "x_j = \\sum_{i\\neq j} a_i x_i\n",
    "$$\n",
    "Since $x_j$ is determined by the other data points then so is $w^\\intercal x_j$ for any $w$. This means the classification on point $x_j$ is dictated by the classification of the other data points and thus cannot freely be chosen.\n",
    "i.e.\n",
    "$$\n",
    "w^\\intercal x_j = w^\\intercal \\sum_{i\\neq j} a_i x_i =\\sum_{i\\neq j} a_i w^\\intercal x_i\n",
    "$$\n",
    "Define an impossible dichotomy as follows. \n",
    "$$\n",
    "y_i = \\textrm{sign}(a_i), \\quad i\\neq j, \\quad y_j = -1\n",
    "$$\n",
    "Show this dichotomy is impossible!\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1\n",
    "\n",
    "Choose $d+1$ data points of $d+1$ dimensions and let them be the columns of a matrix X s.t.\n",
    "\n",
    "1. $X_{:,1} = 1$ (leftmost column)\n",
    "2. $X_{i,i} = 1$ (diagonal)\n",
    "3. $X_{i,j} = 0$ (in all other entries)\n",
    "\n",
    "Let $y$ be any member of the set $\\{-1,+1\\}^{d+1}$.\n",
    "\n",
    "for all $y$, we want to show that there exists a $w$ s.t. for all $i$, $y_i = w^T X_i$.\n",
    "\n",
    "first, let $i=0$. then by construction, $y_0 = w^T X_0 = w_0$. So choose $y_0=w_0$.\n",
    "\n",
    "now, let $i>0$. then for all such $i$, $y_i = w_T X_i = w_0 + w_i = y_0 + w_i$.\n",
    "\n",
    "Since $y_0$ is already known, choose $w_i$ s.t. $w_i = y_i - y_0$. \n",
    "Thus we obtain $d$ equations each of one variable, for which there are always a solution.\n",
    "\n",
    "So for all $y$, there exists a $w$ s.t. for all $i$, $y_i = w^T X_i$.\n",
    "\n",
    "## 3.2\n",
    "\n",
    "Intuitively, if the classification of one point is dependent on the classification on other points, then all dichotomies cannot be acheived.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 4:  Book Exercises\n",
    "## Exercise 1.11 and 1.12 in the Book \n",
    "(Not problems but exercises inside the text. page 25, 26\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 5: Book Problem 2.18 In short\n",
    "Define\n",
    "$$\n",
    "\\mathcal{H}= \\{h_\\alpha \\mid h_\\alpha(x) = (-1)^{\\lfloor \\alpha\n",
    "          x\\rfloor}, \\alpha \\in {{\\mathbb R}}\\}\n",
    "$$ \n",
    "\n",
    "Show that the VC dimension of ${{\\mathcal H}}$ is infinite (even though there is only one parameter!)\n",
    "\n",
    "Hint: Use the points set\n",
    "$x_1=10,x_2=100,\\dots,x_i = 10^i,\\dots,x_N=10^N$ and show how to implement any dichotomy $y_1,\\dots,y_N \\in \\{-1, +1\\}^N$ (find $\\alpha$ that works).\n",
    "You can safely assume $\\alpha >0$.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 6: Regularization with Weight decay\n",
    "If we use weight decay regularization ($\\lambda||w||^2)$  for some real number $\\lambda$ in Linear Regression what \n",
    "happens to the optimal weight vector if we let $\\lambda \\rightarrow \\infty$? (cost is $\\frac{1}{n} \\|Xw - y\\|^2 + \\lambda \\|w\\|^2$)\n",
    "\n",
    "**Hard:** Can you say something about the changes in behaviour of the optimal solution $w$ as $\\lambda$ decreases from $0$ towards $-\\infty$. When does the optimal cost change from something finite to $-\\infty$?\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 7: Grid Search For Regularization and Validation - Sklearn\n",
    "In this exercise you must we will optimize a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) using regularization and validation.\n",
    "You must use the in grid search module [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) from sklearn.\n",
    "\n",
    "In the cell below we have shown an example of how to use the gridsearch module test two different values for max_depth for a a decision tree for wine classification\n",
    "\n",
    "Your job is to good hyperparameters for decision trees for the breast cancer detection.\n",
    "\n",
    "### Task 1:\n",
    "For the breast cancer data set find the best (or very good) combination of max_depth and  min_samples_split  (cell two below)\n",
    "\n",
    "The **max_depth** parameter controls the max depth of a tree and the deeper the tree the more complex the model.\n",
    "\n",
    "The **min_samples_split** controls how many elements the algorithm that constructs the tree is allowed to try and split.\n",
    "So if a subtree contains less than min_leaf_size elements it many not be split into a larger subtree by the algorithm.\n",
    "\n",
    "\n",
    "### Task 2:\n",
    "- How long time does it take to use grid search validation for $k$ hyperparamers where we test each parameter for $d$ values, and the training algorithm uses f(n) time to train on n data points where we split the data into 5 parts.\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_covtype\n",
    "\n",
    "def show_result(clf):\n",
    "    df = pd.DataFrame(clf.cv_results_)\n",
    "    df = df.sort_values('mean_test_score', ascending=False)\n",
    "    display(df)\n",
    "    print('best parameter found', clf.best_params_)\n",
    "    \n",
    "w_data = load_wine()\n",
    "wine_data = w_data.data\n",
    "wine_labels = w_data.target\n",
    "\n",
    "# grid search validation\n",
    "reg_parameters = {'max_depth': [1, 30]}  # dict with all parameters we need to test\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "clf.fit(wine_data, wine_labels)\n",
    "# code for showing the result\n",
    "bt = show_result(clf)\n",
    "                   "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 30}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>0.887571</td>\n",
       "      <td>0.042437</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000546</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1</td>\n",
       "      <td>{'max_depth': 1}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>2</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.67977</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "1       0.000629      0.000050         0.000225        0.000005   \n",
       "0       0.000546      0.000075         0.000260        0.000009   \n",
       "\n",
       "  param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "1              30  {'max_depth': 30}           0.900000           0.830508   \n",
       "0               1   {'max_depth': 1}           0.566667           0.576271   \n",
       "\n",
       "   split2_test_score  mean_test_score  std_test_score  rank_test_score  \\\n",
       "1           0.932203         0.887571        0.042437                1   \n",
       "0           0.711864         0.618267        0.066299                2   \n",
       "\n",
       "   split0_train_score  split1_train_score  split2_train_score  \\\n",
       "1            1.000000            1.000000            1.000000   \n",
       "0            0.677966            0.672269            0.689076   \n",
       "\n",
       "   mean_train_score  std_train_score  \n",
       "1           1.00000         0.000000  \n",
       "0           0.67977         0.006979  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best parameter found {'max_depth': 30}\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "cancer_data = load_breast_cancer()\n",
    "c_data = cancer_data.data\n",
    "c_labels = cancer_data.target\n",
    "\n",
    "\n",
    "def decisiontree_model_selection(train_data, labels):\n",
    "    clf = None\n",
    "    ### YOUR CODE HERE\n",
    "    reg_parameters = {\n",
    "        'max_depth': [x for x in range(1,30)],\n",
    "        'min_samples_split': [x for x in range(2,30)]\n",
    "    }  # dict with all parameters we need to test\n",
    "    clf = GridSearchCV(DecisionTreeClassifier(), reg_parameters, cv=3, return_train_score=True)\n",
    "    clf.fit(wine_data, wine_labels)\n",
    "\n",
    "    ### END CODE\n",
    "    return clf\n",
    "###\n",
    "clf = decisiontree_model_selection(c_data, c_labels)\n",
    "bt = show_result(clf)\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 9}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>0.060482</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 27}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>0.060482</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 23}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.915537</td>\n",
       "      <td>0.060482</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 21}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.909887</td>\n",
       "      <td>0.056130</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>{'max_depth': 3, 'min_samples_split': 8}</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.909887</td>\n",
       "      <td>0.056130</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974790</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.011884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 26}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>785</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.679770</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 27}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>785</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.679770</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 28}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>785</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.679770</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 29}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>785</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.679770</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.576271</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.618267</td>\n",
       "      <td>0.066299</td>\n",
       "      <td>785</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.689076</td>\n",
       "      <td>0.679770</td>\n",
       "      <td>0.006979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>812 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "63       0.000650      0.000084         0.000223        0.000004   \n",
       "81       0.000587      0.000047         0.000217        0.000002   \n",
       "77       0.000599      0.000023         0.000216        0.000005   \n",
       "75       0.000582      0.000044         0.000249        0.000034   \n",
       "62       0.000653      0.000036         0.000253        0.000045   \n",
       "..            ...           ...              ...             ...   \n",
       "24       0.000442      0.000010         0.000217        0.000004   \n",
       "25       0.000519      0.000043         0.000277        0.000055   \n",
       "26       0.000492      0.000031         0.000246        0.000022   \n",
       "27       0.000464      0.000034         0.000234        0.000026   \n",
       "0        0.000606      0.000123         0.000254        0.000023   \n",
       "\n",
       "   param_max_depth param_min_samples_split  \\\n",
       "63               3                       9   \n",
       "81               3                      27   \n",
       "77               3                      23   \n",
       "75               3                      21   \n",
       "62               3                       8   \n",
       "..             ...                     ...   \n",
       "24               1                      26   \n",
       "25               1                      27   \n",
       "26               1                      28   \n",
       "27               1                      29   \n",
       "0                1                       2   \n",
       "\n",
       "                                       params  split0_test_score  \\\n",
       "63   {'max_depth': 3, 'min_samples_split': 9}           0.950000   \n",
       "81  {'max_depth': 3, 'min_samples_split': 27}           0.950000   \n",
       "77  {'max_depth': 3, 'min_samples_split': 23}           0.950000   \n",
       "75  {'max_depth': 3, 'min_samples_split': 21}           0.950000   \n",
       "62   {'max_depth': 3, 'min_samples_split': 8}           0.950000   \n",
       "..                                        ...                ...   \n",
       "24  {'max_depth': 1, 'min_samples_split': 26}           0.566667   \n",
       "25  {'max_depth': 1, 'min_samples_split': 27}           0.566667   \n",
       "26  {'max_depth': 1, 'min_samples_split': 28}           0.566667   \n",
       "27  {'max_depth': 1, 'min_samples_split': 29}           0.566667   \n",
       "0    {'max_depth': 1, 'min_samples_split': 2}           0.566667   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "63           0.830508           0.966102         0.915537        0.060482   \n",
       "81           0.830508           0.966102         0.915537        0.060482   \n",
       "77           0.830508           0.966102         0.915537        0.060482   \n",
       "75           0.830508           0.949153         0.909887        0.056130   \n",
       "62           0.830508           0.949153         0.909887        0.056130   \n",
       "..                ...                ...              ...             ...   \n",
       "24           0.576271           0.711864         0.618267        0.066299   \n",
       "25           0.576271           0.711864         0.618267        0.066299   \n",
       "26           0.576271           0.711864         0.618267        0.066299   \n",
       "27           0.576271           0.711864         0.618267        0.066299   \n",
       "0            0.576271           0.711864         0.618267        0.066299   \n",
       "\n",
       "    rank_test_score  split0_train_score  split1_train_score  \\\n",
       "63                1            1.000000            1.000000   \n",
       "81                1            1.000000            1.000000   \n",
       "77                1            1.000000            1.000000   \n",
       "75                4            1.000000            1.000000   \n",
       "62                4            1.000000            1.000000   \n",
       "..              ...                 ...                 ...   \n",
       "24              785            0.677966            0.672269   \n",
       "25              785            0.677966            0.672269   \n",
       "26              785            0.677966            0.672269   \n",
       "27              785            0.677966            0.672269   \n",
       "0               785            0.677966            0.672269   \n",
       "\n",
       "    split2_train_score  mean_train_score  std_train_score  \n",
       "63            0.974790          0.991597         0.011884  \n",
       "81            0.974790          0.991597         0.011884  \n",
       "77            0.974790          0.991597         0.011884  \n",
       "75            0.974790          0.991597         0.011884  \n",
       "62            0.974790          0.991597         0.011884  \n",
       "..                 ...               ...              ...  \n",
       "24            0.689076          0.679770         0.006979  \n",
       "25            0.689076          0.679770         0.006979  \n",
       "26            0.689076          0.679770         0.006979  \n",
       "27            0.689076          0.679770         0.006979  \n",
       "0             0.689076          0.679770         0.006979  \n",
       "\n",
       "[812 rows x 18 columns]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best parameter found {'max_depth': 3, 'min_samples_split': 9}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}