{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Theoretical Exercises - Week 2\n",
    "**Like last week, it is very imporant that you try to solve every exercise. It is not important that you answer correctly. Spend no more than 5-10 min on each exercise. If you do not solve the exercise, focus on understanding the question, and try to figure out what it is you do not understand.**\n",
    "\n",
    "The TA's will be very happy to answer questions during the TA session or on the board.\n",
    "\n",
    "Do not despair if you cannot solve them, but try to understand the question and pinpoint which parts you do not understand. \n",
    "\n",
    "# 1. Learning Types\n",
    "In this exercise you must distinguish between Supervised Learning and Unsupervised Learning. Imagine you work at a company that sells *stuff*. The company stores information about its costumers. For each costumer the company saves the following 5 attributes:\n",
    "\n",
    "    AGE, SEX, INCOME, RESIDENCE, MONEY USED AT COMPANY\n",
    "\n",
    "<b>Question 1: </b><br>In each of the following examples you should determine if the problem is a Supervised or Unsupervised learning problem.\n",
    "\n",
    "-   The company wants to learn how to predict 'MONEY USED AT COMPANY' given 'AGE', 'SEX', 'INCOME' and 'RESIDENCE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn ways of grouping costumers depending on 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to learn how to predict 'SEX' given 'MONEY SPENT AT COMPANY' and 'AGE'. Supervised or Unsupervised?\n",
    "\n",
    "-   The company wants to target different groups of costumers depending on 'AGE', 'INCOME' and 'MONEY SPENT AT COMPANY'. Supervised or Unsupervised?   \n",
    "\n",
    "<br><br>\n",
    "<b>Question 2</b>:<br>\n",
    "In supervised learning the data is of the form $D_{supervised}=\\{(x_1,y_1),...,(x_n,y_n)\\}$. <br>\n",
    "In unsupervised learning we have data of the form $D_{unsupervised}=\\{x_1,...,x_n\\}$.\n",
    "\n",
    "Write the form the data would take in each case from Question 1.\n",
    "\n",
    "HINT: Possible solutions to two of the cases\n",
    "\n",
    "$$D=\\{20\\text{ years},\\;21\\text{ years}, \\;23\\text{ years}, ... \\}$$ \n",
    "$$D=\\{([100\\text{ kr},\\;22\\text{ years}],\\text{ male}),\\;([120\\text{ kr},\\;30\\text{ years}],\\text{ female}), ...\\}$$\n",
    " \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Regression Or Classification\n",
    "\n",
    "<b>Question 1</b>: <br>In each of the following examples you should distinguish between regression and classification.\n",
    "\n",
    "-   In the previous question the company wanted to predict 'MONEY SPENT AT COMPANY' from ('AGE', 'SEX', 'INCOME', 'RESIDENCE'). Is that regression or classification?\n",
    "\n",
    "-   Recognizing the color of wine as white, rose or red. Is that regression or classification?\n",
    "\n",
    "-   Predicting a students grade in machine learning as a function of previous grades (on the 12 scale). Is that regression or classification? \n",
    "    \n",
    "-   Predicting email as spam, normal. Regression or classification?\n",
    "<br><br>\n",
    "    \n",
    "<b>Question 2: </b> <br>\n",
    "In supervised learning we want to approximate an unkown target function $f:X\\rightarrow Y$. In regression we could have $Y=\\mathbb{R}$ and in classification we could have $Y=\\{c_1,...,c_k\\}$.\n",
    "\n",
    "What is $Y$ in the above four cases? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# question 1 \n",
    " - classification\n",
    " - classification\n",
    " - regression\n",
    " - classification \n",
    "\n",
    "# question 2\n",
    " - Y=reals\n",
    " - Y=set\n",
    " - Y=reals\n",
    " - Y=set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Data that is not numbers\n",
    "### Question 1: Spam Filters\n",
    "You are given the task to design a spam filter and you will be using **Linear Classification** and the perceptron algorithm (since that is all we know yet).\n",
    "\n",
    "The input data consists of a list of (email, spam/not spam label), and each email is represented by a variable length text string. \n",
    "Can you train a spam filter using this data using the perceptron algorithm and if so how? What issues do you see and do you have any ideas how they could be adressed? \n",
    "\n",
    "\n",
    "### Question 2: Categorical Features\n",
    "You are solving a problem with machine learning and have decided to use linear classification (Perceptron). \n",
    "One of the data features is categorical and has four unordered values: Apple, Banana, Grape, Mango. \n",
    "\n",
    "How could you use that feature in a linear classification setup? (The data should be a matrix of size $n \\times d$ of real numbers.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Chosing leaf return value in Decision Trees \n",
    "## (Problem 1.12 from 'Learning By Data')\n",
    "\n",
    "Given $y_1\\leq\\cdots\\leq y_n\\in \\mathbb{R}$ find $h\\in \\mathbb{R}$ that on average is closest to $y_1,\\ldots,y_n$ measured by squared distance (least squares). That is, \n",
    "$$\n",
    "h_\\textrm{ls} =\\textrm{arg}\\min_h \\sum_{i=1}^n (h-y_i)^2\n",
    "$$ \n",
    "<b>Question 1: </b>Show that\n",
    "$h_\\textrm{ls} = \\frac{1}{n} \\sum_{i=1}^n y_i$ is the minimizer. \n",
    "\n",
    "HINT: Computing the <a href=\"https://en.wikipedia.org/wiki/Chain_rule\">derivative</a> may be worth the time and strain on your brain.\n",
    "\n",
    "HINT: a local minimum is a global minimum!\n",
    "\n",
    "\n",
    "<b>Question 2: </b>Consider absolute deviation instead of squared distance, i.e.\n",
    "\n",
    "$$h_\\mathrm{mad} =\\textrm{arg}\\min_h \\sum_{i=1}^n |h-y_i|$$ \n",
    "\n",
    "Show tha $h_\\mathrm{mad} = \\mathrm{median}(y_1,\\dots,y_n)$, the median of the $y$ values is the minimizer. \n",
    "\n",
    "HINT: Computing derivative may be usefull but  $|a|$ is not differentiable at zero but you may set it to zero (ask google about subgradients if you are interested).  \n",
    "\n",
    "HINT: You can also argue purely algorithmically by thinking what happens with the cost as we sweep *h* from $-\\infty$ to $\\infty$).\n",
    "\n",
    "HINT: a local minimum is a global minimum!\n",
    "\n",
    "<b>Question 3: </b>What happens to the solutions $h_\\mathrm{mean}, h_\\mathrm{med}$ if we\n",
    "add noise the last element $y_n$, i.e. $y_n = y_n + \\varepsilon$ for\n",
    "$\\varepsilon \\rightarrow \\infty$.\n",
    "\n",
    "\n",
    "\n",
    "Which method is more stable for outliers (data that looks nothing like the remaining data)?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# q1\n",
    "\n",
    "$f(h)=\\sum_{i=1}^{n} (h-y_i)^2$\n",
    "\n",
    "then $f'(h)=\\sum_{i=1}^{n} 2(h-y_i)$\n",
    "\n",
    "$0=f'(h)$\n",
    "\n",
    "$0=\\sum_{i=1}^{n} 2(h-y_i)$\n",
    "\n",
    "$0/2=\\sum_{i=1}^{n} h-y_i$\n",
    "\n",
    "$0=\\sum_{i=1}^{n} h - \\sum_{i=1}^{n} y_i$\n",
    "\n",
    "$0=nh - \\sum_{i=1}^{n} y_i$\n",
    "\n",
    "$nh = \\sum_{i=1}^{n} y_i$\n",
    "\n",
    "$h = 1/n \\sum_{i=1}^{n} y_i$\n",
    "\n",
    "QED\n",
    "\n",
    "# q2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Decision Tree Cost\n",
    "In this exercise we will construct a small classification tree (height 1, one root, two children) by hand, using entropy as the cost to minimize in the learning algorithm.\n",
    "The data $X$ luckily only has 2 features, and the labels $Y$ are 0,1 encoded  and are as follows:\n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "  1.  & 9. \\\\\n",
    "  2. &33. \\\\\n",
    "  3. &20. \\\\\n",
    "  4. &27. \\\\\n",
    "  6. & 3. \\\\\n",
    "  7. & 6. \\\\\n",
    "  5. &18. \\\\\n",
    "  8. &14. \\\\\n",
    "  9. &16. \\\\\n",
    "  \\end{bmatrix}\n",
    ", \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  0 \\\\ \n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "  1 \\\\\n",
    "\\end{bmatrix} $$\n",
    "  \n",
    "\n",
    "\n",
    "**Task:** \n",
    "  - Compute the entropy cost of the best split for each feature of the data \n",
    "  - Which feature is used in the root? and what value are we comparing to (root uses <=)\n",
    "  - Draw The full tree, including the values returned in the leafs\n",
    "\n",
    "You can use the cell below for computation\n",
    "\n",
    "Remember that the entropy of random variable X taking k values with probability  $p_0, p_1,..., p_{k-1}$ is \n",
    "$H(X) = - \\sum_{i=0}^{k-1} p_i \\log_2 p_i$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "x = np.c_[[1, 2, 3, 4, 6, 7, 5, 8, 9], [9, 33, 20, 27, 3, 6, 18, 14, 16]]\n",
    "y = np.array([0,0 ,0 ,0 ,0 ,0 , 1, 1 ,1])\n",
    "data2 = np.array([[1,9,0],[2,33,0],[3,20,0],[4,27,0],[6,3,0],[7,6,0],[5,18,1],[8,14,1],[9,16,1]])\n",
    "#y = np.array([0,0,0,0,0,0,1,1,1])\n",
    "\n",
    "data = x\n",
    "labels = y\n",
    "\n",
    "print('data matrix shape:', data.shape)\n",
    "print('labels shape:', labels.shape)\n",
    "print(data)\n",
    "print(labels)\n",
    "### YOUR CODE HERE\n",
    "\n",
    "def entropy(labels):\n",
    "    def log2(n):\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return np.log2(n)\n",
    "    \n",
    "    def p(label, labels):\n",
    "        return sum([1 if l == label else 0 for l in labels]) / len(labels)\n",
    "    \n",
    "    res = 0\n",
    "    for i in [0, 1]: # label types\n",
    "        p_i = p(i, labels)\n",
    "        res += p_i * log2(p_i)\n",
    "    return -res\n",
    "\n",
    "def test_cut(labels, cut):\n",
    "    return (entropy(labels[:cut]) * len(labels[:cut]) + entropy(labels[cut:]) * len(labels[cut:])) /len(labels)\n",
    "\n",
    "def cut(data2, d, v):\n",
    "    l1, l2 = [], []\n",
    "    for el in data2:\n",
    "        if el[d] <= v:\n",
    "            l1.append(el[2])\n",
    "        else:\n",
    "            l2.append(el[2])\n",
    "    return l1, l2\n",
    "\n",
    "best = (0,0,1000)\n",
    "for d in range(len(data2[0])-1):\n",
    "    data2 = sorted(data2, key=lambda x : x[d])\n",
    "    for n in range(1, len(data2)-1):\n",
    "        val = test_cut([x[2] for x in data2], n)\n",
    "        if val < best[2]:\n",
    "            best = (d, data2[n-1][d], val)\n",
    "\n",
    "print(\"Best option: \\\"is feature #\" + str(best[0]) + \" <= \" + str(best[1]) + \"?\\\" at entropy = \" + str(best[2]))\n",
    "print(\"This cuts set into groups \" + str(cut(data2, best[0], best[1])))\n",
    "\n",
    "### END CODE"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data matrix shape: (9, 2)\n",
      "labels shape: (9,)\n",
      "[[ 1  9]\n",
      " [ 2 33]\n",
      " [ 3 20]\n",
      " [ 4 27]\n",
      " [ 6  3]\n",
      " [ 7  6]\n",
      " [ 5 18]\n",
      " [ 8 14]\n",
      " [ 9 16]]\n",
      "[0 0 0 0 0 0 1 1 1]\n",
      "Best option: \"is feature #0 <= 7?\" at entropy = 0.4601899388973658\n",
      "This cuts set into groups ([0, 0, 0, 1, 0, 0, 0], [1, 1])\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 6: Classification Stumps in O(n d lg n ) time\n",
    "In this exercise your job is to describe an algorithm that given a data set of labelled data (two classses only), constructs the binary classification tree (one internal node and two leafs) that minimize the 0-1 Loss over the training data. Such small classification trees are called classification stumps.\n",
    "\n",
    "I.e. Given data \n",
    "$$ D = \\{(x_i, y_i) \\mid 1\\leq i \\leq n, y_i \\in \\{0, 1\\}, x_i \\in \\mathbb{R}^d\\}$$\n",
    "construct the binary classification tree $T$ that minimizes \n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^n 1_{T(x_i) \\neq y_i}$$\n",
    "\n",
    "Your algorithm must only use $O(n d \\log n)$ time. \n",
    "\n",
    "The root node considers only questions like $f_i < 42$ and may this be represented by the features index i and the value to compare with (42 here).\n",
    "\n",
    "\n",
    "**Hint: Consider each feature in turn and sort the data for that feature and permute the labels $y$ with the same ordering and compute the score for each relevant split with the given in $O(n \\lg n)$ time**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# algorithm:\n",
    "\n",
    "for each feature d:\n",
    " - sort data, labels w.r.t. feature d\n",
    " - sweep over the sorted data, keeping track of zeroes left of the split, ones left of the split, zeroes right of the split and ones right of the split. With these 4, 0-1 loss can be computed in constant time.\n",
    "return best split found.\n",
    "\n",
    "\n",
    "# analysis:\n",
    "\n",
    "loop runs O(d) times.\n",
    "\n",
    "sorting data and labels takes O(n log(n)) time.\n",
    "sweeping over sorted data takes O(n) time.\n",
    "\n",
    "In total, we arrive at O(d n log n) time, as requested."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ex 7: Implementing Regression Stumps\n",
    "In this exercise your task is to implement Regression Trees that consist of one internal node (the root) and two leafs.\n",
    "Such trees are known as Regression Stumps.\n",
    "For the loss/cost function we consider least squares loss $(h(x) - y)^2$ \n",
    "\n",
    "This means that the learning algorithm has to find the best possible feature to split the training data using a single feature value pair in regards to Least Squares loss.\n",
    "\n",
    "We have decided for you to present a Regression Stump by \n",
    "- idx: the data/feature vector index to consider in the root node (the one question asked)\n",
    "- val: the value to compare to for data feature idx in the root node\n",
    "- left: the value to return for a data point if it ends up in left leaf (x[idx] < val) (only question type we consider in a node)\n",
    "- right: the value to return for a data point if it ends up in the right leaf (x[idx] >= val)\n",
    "\n",
    "The approach we follow is as follows. Assume the input data has n data points each a vector of $d$ real numbers.\n",
    "\n",
    "**Basic Algorithm:**\n",
    "\n",
    "For each data feature f:\n",
    "-   Compute for all possible values $v$ for feature $f$ in the training data, the least squares cost of the stump achieved by using feature f and value $v$ in the root using the optimal value in the two leafs. This gives a list of of costs, one for each split ($f, v: \\textrm{cost}$). \n",
    "- Pick the split $f, v$ with minimal cost and create the corresponding tree by setting idx, val, left, right\n",
    " \n",
    "Your task is to give a full implementation of this algorithm and specify the running time.\n",
    "\n",
    "**hint:** It is fine to implement a simple version for finding the best split that takes $O(d n^2)$ time.\n",
    "\n",
    "See **regression_stumps.py** for starter code.\n",
    "\n",
    "**You need to complete the RegressionStump class by completing the following methods**\n",
    "- implement predict \n",
    "- implement score\n",
    "- implement fit\n",
    "\n",
    "We advice to implement in the order specified.\n",
    "\n",
    "The following hint may lead you to a faster algorithm.\n",
    "$$ \n",
    "\\sum_{i=1}^n (x_i - \\mu)^2 = \\sum_i x_i^2 + \\sum_i \\mu^2 - 2 \\mu \\sum_i x_i = \\sum_i x_i^2 + n \\cdot \\mu^2 - 2 \\mu \\sum_i x_i \n",
    "$$\n",
    "\n",
    "**Explain how to use this hint to get an algorithm for constructing the Regression Stump that runs in $O(d n \\lg n)$ time (you do not need to code it).**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pdb \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from io import StringIO  \n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "\n",
    "\n",
    "def plot_tree(dtree, feature_names):\n",
    "    \"\"\" helper function \"\"\"\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names=feature_names)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    print('exporting tree to dtree.png')\n",
    "    graph.write_png('dtree.png')\n",
    "\n",
    "\n",
    "class RegressionStump():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" The state variables of a stump\"\"\"\n",
    "        self.idx = None\n",
    "        self.val = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "    \n",
    "    def fit(self, data, targets):\n",
    "        \"\"\" Fit a decision stump to data\n",
    "        \n",
    "        Find the best way to split the data in feat  minimizig the cost (0-1) loss of the tree after the split \n",
    "    \n",
    "        Args:\n",
    "           data: np.array (n, d)  features\n",
    "           targets: np.array (n, ) targets\n",
    "    \n",
    "        sets self.idx, self.val, self.left, self.right\n",
    "        \"\"\"\n",
    "        # update these three\n",
    "        self.idx = 0\n",
    "        self.val = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Regression tree prediction algorithm\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "        \n",
    "        returns pred: np.array shape n,  model prediction on X\n",
    "        \"\"\"\n",
    "        pred = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute accuracy of model\n",
    "\n",
    "        Args\n",
    "            X: np.array, shape n,d\n",
    "            y: np.array, shape n, \n",
    "\n",
    "        returns out: scalar - means least scores cost\n",
    "        \"\"\"\n",
    "        out = None\n",
    "        ### YOUR CODE HERE\n",
    "        ### END CODE\n",
    "        return out\n",
    "        \n",
    "\n",
    "### YOUR CODE HERE\n",
    "### END CODE\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\" Simple method testing \"\"\"\n",
    "    boston = load_boston()\n",
    "    # split 80/20 train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(boston.data,\n",
    "                                                        boston.target,\n",
    "                                                        test_size=0.2)\n",
    "\n",
    "    baseline_accuracy = np.mean((y_test-np.mean(y_train))**2)\n",
    "    print('Least Squares Cost of learning mean of training data:', baseline_accuracy) \n",
    "    print('Lets see if we can do better with just one question')\n",
    "    D = RegressionStump()\n",
    "    D.fit(X_train, y_train)\n",
    "    print('idx, val, left, right', D.idx, D.val, D.left, D.right)\n",
    "    print('Feature name of idx', boston.feature_names[D.idx])\n",
    "    print('Score of model', D.score(X_test, y_test))\n",
    "    print('lets compare with sklearn decision tree')\n",
    "    dc = DecisionTreeRegressor(max_depth=1)\n",
    "    dc.fit(X_train, y_train)\n",
    "    dc_score = ((dc.predict(X_test)-y_test)**2).mean()\n",
    "    print('dc score', dc_score)\n",
    "    print('feature names - for comparison', list(enumerate(boston.feature_names)))\n",
    "    plot_tree(dc, boston.feature_names)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Least Squares Cost of learning mean of training data: 80.70831753807295\n",
      "Lets see if we can do better with just one question\n",
      "idx, val, left, right 0 None None None\n",
      "Feature name of idx CRIM\n",
      "Score of model None\n",
      "lets compare with sklearn decision tree\n",
      "dc score 45.15303964763167\n",
      "feature names - for comparison [(0, 'CRIM'), (1, 'ZN'), (2, 'INDUS'), (3, 'CHAS'), (4, 'NOX'), (5, 'RM'), (6, 'AGE'), (7, 'DIS'), (8, 'RAD'), (9, 'TAX'), (10, 'PTRATIO'), (11, 'B'), (12, 'LSTAT')]\n",
      "exporting tree to dtree.png\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16652/2850242056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16652/2850242056.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dc score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature names - for comparison'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboston\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16652/2850242056.py\u001b[0m in \u001b[0;36mplot_tree\u001b[0;34m(dtree, feature_names)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exporting tree to dtree.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dtree.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path, f, prog)\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m                 \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m             )\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, path, prog, format)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m                 \u001b[0mfobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pydotplus/graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format)\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m                 raise InvocationException(\n\u001b[0m\u001b[1;32m   1960\u001b[0m                     'GraphViz\\'s executables not found')\n\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}